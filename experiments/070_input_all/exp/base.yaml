seed: 7

modes: [train, valid, test, viz]


dataset_dir: input/make_webdataset_batch/all
scale_dir: misc/normalize_v7_feat
norm_name: "1e+60"
test_path: input/test.parquet
sample_submission_path: input/sample_submission.parquet
viz_notebook_path: notebook/result_viz_002_for_v5.ipynb
pred_checkpoint_path: null


# data 指定期間内のデータを利用。skip_modで間引く
train_start: [1, 2] #1の2月
train_end: [8, 1]
valid_start: [8, 2]
valid_end: [9, 1]

train_data_skip_mod: 32
valid_data_skip_mod: 7

eps: 1e-60
outlier_std_rate: 40
additional_replace_target: [] # ${cols.weight_zero_index_list}　以外で必要あれば
fill_target: [
  ptend_q0002_12,
  ptend_q0002_13,
  ptend_q0002_14,
  ptend_q0002_15,
  ptend_q0002_16,
  ptend_q0002_17,
  ptend_q0002_18,
  ptend_q0002_19,
  ptend_q0002_20,
  ptend_q0002_21,
  ptend_q0002_22,
  ptend_q0002_23,
  ptend_q0002_24,
  ptend_q0002_25,
  ptend_q0002_26,
  ptend_q0002_27,
]


seq_feats: [
  water,
]
scalar_feats: [
]



# model
model:
  same_height_hidden_sizes: [128, 128]
  embedding_dim: 10
  categorical_embedding_dim: 5
  dropout: 0.2
  bilinear: False
  n_base_channels: 128



norm_seq: False

# Training
max_epochs: 20
early_stopping_patience: 2
num_workers: 8
train_batch_size: 1
valid_batch_size: 1

# LightningModule
scheduler:
  use_one_epoch_warmup: True
ema:
  use_ema: True
  decay: 0.9975
optimizer: 
  name: Adan
  lr: 5e-3
  weight_decay: 0.02
  eps: 1e-8
  opt_betas: [0.98, 0.92, 0.99]
  max_grad_norm: 0.0
  no_prox: False



# trainer
accelerator: auto
precision: "16-mixed" # https://lightning.ai/docs/pytorch/stable/common/trainer.html#precision
gradient_clip_val: 1.0
accumulate_grad_batches: 4
resume_ckpt_path: null
val_check_interval: null # 学習データ量増やしたら設定しておくと良さそう
val_check_warmup: 5000
