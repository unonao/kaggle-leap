seed: 7

modes: [train, valid, test, viz]


dataset_dir: input/make_webdataset_batch/all
scale_dir: misc/normalize_v5
norm_name: "0"
test_path: input/test.parquet
sample_submission_path: input/sample_submission.parquet
viz_notebook_path: notebook/result_viz_002_for_v5.ipynb
pred_checkpoint_path: null


# data 指定期間内のデータを利用。skip_modで間引く
train_start: [1, 2] #1の2月
train_end: [8, 1]
valid_start: [8, 2]
valid_end: [9, 1]

train_data_skip_mod: 32
valid_data_skip_mod: 7

eps: 1e-60
outlier_std_rate: 10
additional_replace_target: [] # ${cols.weight_zero_index_list}　以外で必要あれば
fill_target: [
  ptend_q0002_12,
  ptend_q0002_13,
  ptend_q0002_14,
  ptend_q0002_15,
  ptend_q0002_16,
  ptend_q0002_17,
  ptend_q0002_18,
  ptend_q0002_19,
  ptend_q0002_20,
  ptend_q0002_21,
  ptend_q0002_22,
  ptend_q0002_23,
  ptend_q0002_24,
  ptend_q0002_25,
  ptend_q0002_26,
  ptend_q0002_27,
]


# model
model:
  same_height_hidden_sizes: [120, 60] # 6個にpoolingするので 最終層の次元数は6の倍数にする
  embedding_dim: 10
  last_pooling: avg # max, linear
  dropout: 0.2
  bilinear: False
  n_base_channels: 32


norm_seq: False

# Training
max_epochs: 20
early_stopping_patience: 2
num_workers: 8
train_batch_size: 1
valid_batch_size: 1

# LightningModule
scheduler:
  use_one_epoch_warmup: True
ema:
  use_ema: True
  decay: 0.9975
optimizer: 
  name: AdamW
  lr: 5e-3
  weight_decay: 0.01

# trainer
accelerator: auto
precision: "32" # https://lightning.ai/docs/pytorch/stable/common/trainer.html#precision
gradient_clip_val: 1.0
accumulate_grad_batches: 4
resume_ckpt_path: null
val_check_interval: null # 学習データ量増やしたら設定しておくと良さそう

