{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5597cd12-3396-42d9-a908-7b2b72d9a02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 00:00:22.732966: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-29 00:00:22.733107: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-29 00:00:22.868582: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-29 00:00:23.142329: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working\n",
    "\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from typing import Literal\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "959d7fa1-709e-427d-9b95-4774c74f4d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable names\n",
    "\n",
    "v2_inputs = [\n",
    "    \"state_t\",\n",
    "    \"state_q0001\",\n",
    "    \"state_q0002\",\n",
    "    \"state_q0003\",\n",
    "    \"state_u\",\n",
    "    \"state_v\",\n",
    "    \"state_ps\",\n",
    "    \"pbuf_SOLIN\",\n",
    "    \"pbuf_LHFLX\",\n",
    "    \"pbuf_SHFLX\",\n",
    "    \"pbuf_TAUX\",\n",
    "    \"pbuf_TAUY\",\n",
    "    \"pbuf_COSZRS\",\n",
    "    \"cam_in_ALDIF\",\n",
    "    \"cam_in_ALDIR\",\n",
    "    \"cam_in_ASDIF\",\n",
    "    \"cam_in_ASDIR\",\n",
    "    \"cam_in_LWUP\",\n",
    "    \"cam_in_ICEFRAC\",\n",
    "    \"cam_in_LANDFRAC\",\n",
    "    \"cam_in_OCNFRAC\",\n",
    "    \"cam_in_SNOWHICE\",\n",
    "    \"cam_in_SNOWHLAND\",\n",
    "    \"pbuf_ozone\",  # outside of the upper troposphere lower stratosphere (UTLS, corresponding to indices 5-21), variance in minimal for these last 3\n",
    "    \"pbuf_CH4\",\n",
    "    \"pbuf_N2O\",\n",
    "]\n",
    "\n",
    "\n",
    "v2_outputs = [\n",
    "    \"ptend_t\",\n",
    "    \"ptend_q0001\",\n",
    "    \"ptend_q0002\",\n",
    "    \"ptend_q0003\",\n",
    "    \"ptend_u\",\n",
    "    \"ptend_v\",\n",
    "    \"cam_out_NETSW\",\n",
    "    \"cam_out_FLWDS\",\n",
    "    \"cam_out_PRECSC\",\n",
    "    \"cam_out_PRECC\",\n",
    "    \"cam_out_SOLS\",\n",
    "    \"cam_out_SOLL\",\n",
    "    \"cam_out_SOLSD\",\n",
    "    \"cam_out_SOLLD\",\n",
    "]\n",
    "vertically_resolved = [\n",
    "    \"state_t\",\n",
    "    \"state_q0001\",\n",
    "    \"state_q0002\",\n",
    "    \"state_q0003\",\n",
    "    \"state_u\",\n",
    "    \"state_v\",\n",
    "    \"pbuf_ozone\",\n",
    "    \"pbuf_CH4\",\n",
    "    \"pbuf_N2O\",\n",
    "    \"ptend_t\",\n",
    "    \"ptend_q0001\",\n",
    "    \"ptend_q0002\",\n",
    "    \"ptend_q0003\",\n",
    "    \"ptend_u\",\n",
    "    \"ptend_v\",\n",
    "]\n",
    "\n",
    "ablated_vars = [\"ptend_q0001\", \"ptend_q0002\", \"ptend_q0003\", \"ptend_u\", \"ptend_v\"]\n",
    "\n",
    "v2_vars = v2_inputs + v2_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a4cc67-5e0d-41ba-a5e4-6ea7174e25d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(925, 60)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_col_names = []\n",
    "ablated_col_names = []\n",
    "for var in v2_vars:\n",
    "    if var in vertically_resolved:\n",
    "        for i in range(60):\n",
    "            train_col_names.append(var + \"_\" + str(i))\n",
    "            if i < 12 and var in ablated_vars:\n",
    "                ablated_col_names.append(var + \"_\" + str(i))\n",
    "    else:\n",
    "        train_col_names.append(var)\n",
    "\n",
    "len(train_col_names), len(ablated_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a851baf-df9e-4987-af9c-ff0c25d74e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_col_names = []\n",
    "for var in v2_inputs:\n",
    "    if var in vertically_resolved:\n",
    "        for i in range(60):\n",
    "            input_col_names.append(var + \"_\" + str(i))\n",
    "    else:\n",
    "        input_col_names.append(var)\n",
    "len(input_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd2aed5e-6a36-4ea9-86da-7616863c9ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_col_names = []\n",
    "for var in v2_outputs:\n",
    "    if var in vertically_resolved:\n",
    "        for i in range(60):\n",
    "            output_col_names.append(var + \"_\" + str(i))\n",
    "    else:\n",
    "        output_col_names.append(var)\n",
    "len(output_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e492486-b619-4e88-956a-f500f9efa71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_col_names) == 17 + 60 * 9 + 60 * 6 + 8\n",
    "assert len(input_col_names) == 17 + 60 * 9\n",
    "assert len(output_col_names) == 60 * 6 + 8\n",
    "assert len(set(output_col_names).intersection(set(ablated_col_names))) == len(\n",
    "    ablated_col_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c30aaed-cfa1-45d8-8913-b72cd2407450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f10a635c-f8f7-4ed9-b0c6-0f3594dedd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_path = \"/kaggle/working/misc/grid_info/ClimSim_low-res_grid-info.nc\"\n",
    "norm_path = \"/kaggle/working/misc/preprocessing/normalizations/\"\n",
    "grid_info = xr.open_dataset(grid_path)\n",
    "input_mean = xr.open_dataset(norm_path + \"inputs/input_mean.nc\")\n",
    "input_max = xr.open_dataset(norm_path + \"inputs/input_max.nc\")\n",
    "input_min = xr.open_dataset(norm_path + \"inputs/input_min.nc\")\n",
    "output_scale = xr.open_dataset(norm_path + \"outputs/output_scale.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fe14ecc-293d-413b-b719-1d8a5d3ad9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 5kB\n",
       "Dimensions:           (lev: 60)\n",
       "Dimensions without coordinates: lev\n",
       "Data variables: (12/27)\n",
       "    cam_in_ALDIF      float64 8B 0.5598\n",
       "    cam_in_ALDIR      float64 8B ...\n",
       "    cam_in_ASDIF      float64 8B ...\n",
       "    cam_in_ASDIR      float64 8B ...\n",
       "    cam_in_ICEFRAC    float64 8B ...\n",
       "    cam_in_LANDFRAC   float64 8B ...\n",
       "    ...                ...\n",
       "    state_q0001       (lev) float64 480B ...\n",
       "    state_q0002       (lev) float64 480B ...\n",
       "    state_q0003       (lev) float64 480B ...\n",
       "    state_t           (lev) float64 480B ...\n",
       "    state_u           (lev) float64 480B ...\n",
       "    state_v           (lev) float64 480B ...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-fa116b0b-7a8d-48cc-bdd5-962723fea23e' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-fa116b0b-7a8d-48cc-bdd5-962723fea23e' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span>lev</span>: 60</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-091169a7-e6a0-4c10-8d68-be52f445bd00' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-091169a7-e6a0-4c10-8d68-be52f445bd00' class='xr-section-summary'  title='Expand/collapse section'>Coordinates: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-f7639535-79b4-4349-8a60-5b049421fcf8' class='xr-section-summary-in' type='checkbox'  ><label for='section-f7639535-79b4-4349-8a60-5b049421fcf8' class='xr-section-summary' >Data variables: <span>(27)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>cam_in_ALDIF</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.5598</div><input id='attrs-9f9cd903-1e74-4835-8544-d777432a89c4' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-9f9cd903-1e74-4835-8544-d777432a89c4' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-9e6ec83e-ae85-4803-bd8e-ea5928d0de55' class='xr-var-data-in' type='checkbox'><label for='data-9e6ec83e-ae85-4803-bd8e-ea5928d0de55' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(0.559801)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_in_ALDIR</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-6e9109a9-cd14-4296-80de-128158563708' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-6e9109a9-cd14-4296-80de-128158563708' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-0ae495fb-908c-4972-9a3e-30e906dd6ebd' class='xr-var-data-in' type='checkbox'><label for='data-0ae495fb-908c-4972-9a3e-30e906dd6ebd' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_in_ASDIF</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-2366fb7f-f2e5-4d27-b64b-2c715752477e' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-2366fb7f-f2e5-4d27-b64b-2c715752477e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1c12bff5-fe23-479a-8c7a-4947fee83be2' class='xr-var-data-in' type='checkbox'><label for='data-1c12bff5-fe23-479a-8c7a-4947fee83be2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_in_ASDIR</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-840fbf14-dd1c-498c-991b-6dbb597d3af7' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-840fbf14-dd1c-498c-991b-6dbb597d3af7' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-2d53ef3d-1c0d-4acf-8ae8-4b305757133c' class='xr-var-data-in' type='checkbox'><label for='data-2d53ef3d-1c0d-4acf-8ae8-4b305757133c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_in_ICEFRAC</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-1e50fcdc-a9a7-4527-94f7-6d686f79fd17' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-1e50fcdc-a9a7-4527-94f7-6d686f79fd17' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-c0f1acfa-b872-445b-9177-9e308e52736b' class='xr-var-data-in' type='checkbox'><label for='data-c0f1acfa-b872-445b-9177-9e308e52736b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_in_LANDFRAC</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-08be91fd-e920-49e9-b58e-725bcfbe2c2e' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-08be91fd-e920-49e9-b58e-725bcfbe2c2e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-bd24ad20-fe8e-4364-8293-61dd89a592b3' class='xr-var-data-in' type='checkbox'><label for='data-bd24ad20-fe8e-4364-8293-61dd89a592b3' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_in_LWUP</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-777227d7-6a45-4450-bf92-9b5d43522f76' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-777227d7-6a45-4450-bf92-9b5d43522f76' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d246442b-276e-4404-8255-bcb537fb0311' class='xr-var-data-in' type='checkbox'><label for='data-d246442b-276e-4404-8255-bcb537fb0311' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_in_OCNFRAC</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-a49a3f88-acf6-4f1f-baf2-79444012b97c' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-a49a3f88-acf6-4f1f-baf2-79444012b97c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-22ec4073-224c-44fa-9883-1be241d5ff44' class='xr-var-data-in' type='checkbox'><label for='data-22ec4073-224c-44fa-9883-1be241d5ff44' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_in_SNOWHICE</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-e11dc639-225b-4885-b588-06acfe0e4c14' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-e11dc639-225b-4885-b588-06acfe0e4c14' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-9f0d308e-df1e-4089-a646-49440f655341' class='xr-var-data-in' type='checkbox'><label for='data-9f0d308e-df1e-4089-a646-49440f655341' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>cam_in_SNOWHLAND</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-931a9653-cd16-4e5c-bc3c-a10960a1637a' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-931a9653-cd16-4e5c-bc3c-a10960a1637a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-c9bea0a1-5374-4d4e-bee4-aaba0a8397e8' class='xr-var-data-in' type='checkbox'><label for='data-c9bea0a1-5374-4d4e-bee4-aaba0a8397e8' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>pbuf_CH4</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-5ea4be1f-288a-460d-b4a7-300d4fc4d310' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-5ea4be1f-288a-460d-b4a7-300d4fc4d310' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-be015189-cece-4488-9b48-c89e82cb43c1' class='xr-var-data-in' type='checkbox'><label for='data-be015189-cece-4488-9b48-c89e82cb43c1' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[60 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>pbuf_COSZRS</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-8f58eaa5-9395-4842-9646-e42cf0d9ef88' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-8f58eaa5-9395-4842-9646-e42cf0d9ef88' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-0f1004a8-687e-48ed-8f19-a715b182fa05' class='xr-var-data-in' type='checkbox'><label for='data-0f1004a8-687e-48ed-8f19-a715b182fa05' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>pbuf_LHFLX</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-93b4768e-e624-4d48-9e30-bde1ecc90cd1' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-93b4768e-e624-4d48-9e30-bde1ecc90cd1' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-83a6bdd1-c618-4f7e-848a-a1ee912bf069' class='xr-var-data-in' type='checkbox'><label for='data-83a6bdd1-c618-4f7e-848a-a1ee912bf069' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>pbuf_N2O</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-818e51ea-ee7a-49e6-8c9c-433097feda16' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-818e51ea-ee7a-49e6-8c9c-433097feda16' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-16c9f26a-0abc-4d8b-b956-c682cb84d3e6' class='xr-var-data-in' type='checkbox'><label for='data-16c9f26a-0abc-4d8b-b956-c682cb84d3e6' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[60 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>pbuf_SHFLX</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-21cb648b-db9a-4cdc-956d-14e73f5c112a' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-21cb648b-db9a-4cdc-956d-14e73f5c112a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1a227da2-2bb7-4973-8110-cc89cd4ebab0' class='xr-var-data-in' type='checkbox'><label for='data-1a227da2-2bb7-4973-8110-cc89cd4ebab0' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>pbuf_SOLIN</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-454bbe24-ad26-48aa-a1eb-2518db5c9352' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-454bbe24-ad26-48aa-a1eb-2518db5c9352' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-c486c2dd-f91b-4c7e-a77d-0b1513fdfd0b' class='xr-var-data-in' type='checkbox'><label for='data-c486c2dd-f91b-4c7e-a77d-0b1513fdfd0b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>pbuf_TAUX</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-fedb91d9-e1d5-4c66-8fff-64dcde5602c1' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-fedb91d9-e1d5-4c66-8fff-64dcde5602c1' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-69dd1250-0dde-407e-bee8-91820cc1f251' class='xr-var-data-in' type='checkbox'><label for='data-69dd1250-0dde-407e-bee8-91820cc1f251' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>pbuf_TAUY</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-e0294ba5-e900-44f3-b285-a2f8db551570' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-e0294ba5-e900-44f3-b285-a2f8db551570' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-83a85a4f-a073-4537-b1d3-c8e9110207ec' class='xr-var-data-in' type='checkbox'><label for='data-83a85a4f-a073-4537-b1d3-c8e9110207ec' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>pbuf_ozone</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-0fa09d98-4362-4b65-bdc8-4dd94026b9fa' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-0fa09d98-4362-4b65-bdc8-4dd94026b9fa' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-3aa7a983-bdef-4e5a-9495-3384a6b8181b' class='xr-var-data-in' type='checkbox'><label for='data-3aa7a983-bdef-4e5a-9495-3384a6b8181b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[60 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>state_pmid</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-758970ca-18f9-4f97-93a3-19cfbdba33ec' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-758970ca-18f9-4f97-93a3-19cfbdba33ec' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f3c412b5-884a-4df6-bc64-b45acf287089' class='xr-var-data-in' type='checkbox'><label for='data-f3c412b5-884a-4df6-bc64-b45acf287089' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[60 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>state_ps</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-62fd8293-1978-49a9-859a-408c0cbb4ce5' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-62fd8293-1978-49a9-859a-408c0cbb4ce5' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-23c3d140-c2b7-4f1b-928f-3a8799daa330' class='xr-var-data-in' type='checkbox'><label for='data-23c3d140-c2b7-4f1b-928f-3a8799daa330' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>state_q0001</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-e2df7eeb-c704-4875-868f-25e53a622dd9' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-e2df7eeb-c704-4875-868f-25e53a622dd9' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4bb8d683-6789-4f3b-b9df-1682fd56a556' class='xr-var-data-in' type='checkbox'><label for='data-4bb8d683-6789-4f3b-b9df-1682fd56a556' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[60 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>state_q0002</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-710e02bf-ba6f-4fd3-8a49-00c27092ab40' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-710e02bf-ba6f-4fd3-8a49-00c27092ab40' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-18076369-6a6e-4894-b19e-2f1fcfe65f4a' class='xr-var-data-in' type='checkbox'><label for='data-18076369-6a6e-4894-b19e-2f1fcfe65f4a' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[60 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>state_q0003</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-4d7af925-49bf-4c5f-bef8-8f2d9ab99c7a' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-4d7af925-49bf-4c5f-bef8-8f2d9ab99c7a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-572b439d-8e21-47ef-ad09-27c1a1aef6a5' class='xr-var-data-in' type='checkbox'><label for='data-572b439d-8e21-47ef-ad09-27c1a1aef6a5' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[60 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>state_t</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-7ac4647e-e901-497b-8bac-d633995f7d58' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-7ac4647e-e901-497b-8bac-d633995f7d58' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f4a55ba6-5394-45d9-bef4-a932aa8c457f' class='xr-var-data-in' type='checkbox'><label for='data-f4a55ba6-5394-45d9-bef4-a932aa8c457f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[60 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>state_u</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-c83412d9-372b-4740-903a-ee79bda2cea4' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-c83412d9-372b-4740-903a-ee79bda2cea4' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-cc913e50-c0d6-4be4-b85b-1b8616d61123' class='xr-var-data-in' type='checkbox'><label for='data-cc913e50-c0d6-4be4-b85b-1b8616d61123' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[60 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>state_v</span></div><div class='xr-var-dims'>(lev)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-13ef74c1-0477-48b8-ab0c-22e532ebd070' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-13ef74c1-0477-48b8-ab0c-22e532ebd070' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-85dc91cf-f1ee-498a-a23f-1faad3091754' class='xr-var-data-in' type='checkbox'><label for='data-85dc91cf-f1ee-498a-a23f-1faad3091754' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[60 values with dtype=float64]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-3d372fa3-0d39-4ea4-b7b0-1aebb8f87272' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-3d372fa3-0d39-4ea4-b7b0-1aebb8f87272' class='xr-section-summary'  title='Expand/collapse section'>Indexes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-5909f75a-da4e-4e71-99cf-83b74a7bf357' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-5909f75a-da4e-4e71-99cf-83b74a7bf357' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 5kB\n",
       "Dimensions:           (lev: 60)\n",
       "Dimensions without coordinates: lev\n",
       "Data variables: (12/27)\n",
       "    cam_in_ALDIF      float64 8B 0.5598\n",
       "    cam_in_ALDIR      float64 8B ...\n",
       "    cam_in_ASDIF      float64 8B ...\n",
       "    cam_in_ASDIR      float64 8B ...\n",
       "    cam_in_ICEFRAC    float64 8B ...\n",
       "    cam_in_LANDFRAC   float64 8B ...\n",
       "    ...                ...\n",
       "    state_q0001       (lev) float64 480B ...\n",
       "    state_q0002       (lev) float64 480B ...\n",
       "    state_q0003       (lev) float64 480B ...\n",
       "    state_t           (lev) float64 480B ...\n",
       "    state_u           (lev) float64 480B ...\n",
       "    state_v           (lev) float64 480B ..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c94373-a81e-48de-854d-3f543135c675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dc8245e-8004-46bb-861b-8eab66a16be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLBackendType = Literal[\"tensorflow\", \"pytorch\"]\n",
    "\n",
    "\n",
    "class data_utils:\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_info,\n",
    "        input_mean,\n",
    "        input_max,\n",
    "        input_min,\n",
    "        output_scale,\n",
    "        ml_backend: MLBackendType = \"pytorch\",\n",
    "    ):\n",
    "        self.data_path = None\n",
    "        self.input_vars = []\n",
    "        self.target_vars = []\n",
    "        self.input_feature_len = None\n",
    "        self.target_feature_len = None\n",
    "        self.grid_info = grid_info\n",
    "        self.level_name = \"lev\"\n",
    "        self.sample_name = \"sample\"\n",
    "        self.num_levels = len(self.grid_info[\"lev\"])\n",
    "        self.num_latlon = len(\n",
    "            self.grid_info[\"ncol\"]\n",
    "        )  # number of unique lat/lon grid points\n",
    "        # make area-weights\n",
    "        self.grid_info[\"area_wgt\"] = self.grid_info[\"area\"] / self.grid_info[\n",
    "            \"area\"\n",
    "        ].mean(dim=\"ncol\")\n",
    "        self.area_wgt = self.grid_info[\"area_wgt\"].values\n",
    "        # map ncol to nsamples dimension\n",
    "        # to_xarray = {'area_wgt':(self.sample_name,np.tile(self.grid_info['area_wgt'], int(n_samples/len(self.grid_info['ncol']))))}\n",
    "        # to_xarray = xr.Dataset(to_xarray)\n",
    "        self.input_mean = input_mean\n",
    "        self.input_max = input_max\n",
    "        self.input_min = input_min\n",
    "        self.output_scale = output_scale\n",
    "        self.normalize = True\n",
    "        self.lats, self.lats_indices = np.unique(\n",
    "            self.grid_info[\"lat\"].values, return_index=True\n",
    "        )\n",
    "        self.lons, self.lons_indices = np.unique(\n",
    "            self.grid_info[\"lon\"].values, return_index=True\n",
    "        )\n",
    "        self.sort_lat_key = np.argsort(\n",
    "            self.grid_info[\"lat\"].values[np.sort(self.lats_indices)]\n",
    "        )\n",
    "        self.sort_lon_key = np.argsort(\n",
    "            self.grid_info[\"lon\"].values[np.sort(self.lons_indices)]\n",
    "        )\n",
    "        self.indextolatlon = {\n",
    "            i: (\n",
    "                self.grid_info[\"lat\"].values[i % self.num_latlon],\n",
    "                self.grid_info[\"lon\"].values[i % self.num_latlon],\n",
    "            )\n",
    "            for i in range(self.num_latlon)\n",
    "        }\n",
    "\n",
    "        self.ml_backend = ml_backend\n",
    "        self.tf = None\n",
    "        self.torch = None\n",
    "\n",
    "        if self.ml_backend == \"tensorflow\":\n",
    "            self.successful_backend_import = False\n",
    "\n",
    "            try:\n",
    "                import tensorflow as tf\n",
    "\n",
    "                self.tf = tf\n",
    "                self.successful_backend_import = True\n",
    "            except ImportError:\n",
    "                raise ImportError(\"Tensorflow is not installed.\")\n",
    "\n",
    "        elif self.ml_backend == \"pytorch\":\n",
    "            self.successful_backend_import = False\n",
    "\n",
    "            try:\n",
    "                import torch\n",
    "\n",
    "                self.torch = torch\n",
    "                self.successful_backend_import = True\n",
    "            except ImportError:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "\n",
    "        def find_keys(dictionary, value):\n",
    "            keys = []\n",
    "            for key, val in dictionary.items():\n",
    "                if val[0] == value:\n",
    "                    keys.append(key)\n",
    "            return keys\n",
    "\n",
    "        indices_list = []\n",
    "        for lat in self.lats:\n",
    "            indices = find_keys(self.indextolatlon, lat)\n",
    "            indices_list.append(indices)\n",
    "        indices_list.sort(key=lambda x: x[0])\n",
    "        self.lat_indices_list = indices_list\n",
    "\n",
    "        self.hyam = self.grid_info[\"hyam\"].values\n",
    "        self.hybm = self.grid_info[\"hybm\"].values\n",
    "        self.p0 = 1e5  # code assumes this will always be a scalar\n",
    "        self.ps_index = None\n",
    "\n",
    "        self.pressure_grid_train = None\n",
    "        self.pressure_grid_val = None\n",
    "        self.pressure_grid_scoring = None\n",
    "        self.pressure_grid_test = None\n",
    "\n",
    "        self.dp_train = None\n",
    "        self.dp_val = None\n",
    "        self.dp_scoring = None\n",
    "        self.dp_test = None\n",
    "\n",
    "        self.train_regexps = None\n",
    "        self.train_stride_sample = None\n",
    "        self.train_filelist = None\n",
    "        self.val_regexps = None\n",
    "        self.val_stride_sample = None\n",
    "        self.val_filelist = None\n",
    "        self.scoring_regexps = None\n",
    "        self.scoring_stride_sample = None\n",
    "        self.scoring_filelist = None\n",
    "        self.test_regexps = None\n",
    "        self.test_stride_sample = None\n",
    "        self.test_filelist = None\n",
    "\n",
    "        self.full_vars = False\n",
    "\n",
    "        # physical constants from E3SM_ROOT/share/util/shr_const_mod.F90\n",
    "        self.grav = 9.80616  # acceleration of gravity ~ m/s^2\n",
    "        self.cp = 1.00464e3  # specific heat of dry air   ~ J/kg/K\n",
    "        self.lv = 2.501e6  # latent heat of evaporation ~ J/kg\n",
    "        self.lf = 3.337e5  # latent heat of fusion      ~ J/kg\n",
    "        self.lsub = self.lv + self.lf  # latent heat of sublimation ~ J/kg\n",
    "        self.rho_air = (\n",
    "            101325 / (6.02214e26 * 1.38065e-23 / 28.966) / 273.15\n",
    "        )  # density of dry air at STP  ~ kg/m^3\n",
    "        # ~ 1.2923182846924677\n",
    "        # SHR_CONST_PSTD/(SHR_CONST_RDAIR*SHR_CONST_TKFRZ)\n",
    "        # SHR_CONST_RDAIR   = SHR_CONST_RGAS/SHR_CONST_MWDAIR\n",
    "        # SHR_CONST_RGAS    = SHR_CONST_AVOGAD*SHR_CONST_BOLTZ\n",
    "        self.rho_h20 = 1.0e3  # density of fresh water     ~ kg/m^ 3\n",
    "\n",
    "        self.v1_inputs = [\n",
    "            \"state_t\",\n",
    "            \"state_q0001\",\n",
    "            \"state_ps\",\n",
    "            \"pbuf_SOLIN\",\n",
    "            \"pbuf_LHFLX\",\n",
    "            \"pbuf_SHFLX\",\n",
    "        ]\n",
    "\n",
    "        self.v1_outputs = [\n",
    "            \"ptend_t\",\n",
    "            \"ptend_q0001\",\n",
    "            \"cam_out_NETSW\",\n",
    "            \"cam_out_FLWDS\",\n",
    "            \"cam_out_PRECSC\",\n",
    "            \"cam_out_PRECC\",\n",
    "            \"cam_out_SOLS\",\n",
    "            \"cam_out_SOLL\",\n",
    "            \"cam_out_SOLSD\",\n",
    "            \"cam_out_SOLLD\",\n",
    "        ]\n",
    "\n",
    "        self.v2_inputs = [\n",
    "            \"state_t\",\n",
    "            \"state_q0001\",\n",
    "            \"state_q0002\",\n",
    "            \"state_q0003\",\n",
    "            \"state_u\",\n",
    "            \"state_v\",\n",
    "            \"state_ps\",\n",
    "            \"pbuf_SOLIN\",\n",
    "            \"pbuf_LHFLX\",\n",
    "            \"pbuf_SHFLX\",\n",
    "            \"pbuf_TAUX\",\n",
    "            \"pbuf_TAUY\",\n",
    "            \"pbuf_COSZRS\",\n",
    "            \"cam_in_ALDIF\",\n",
    "            \"cam_in_ALDIR\",\n",
    "            \"cam_in_ASDIF\",\n",
    "            \"cam_in_ASDIR\",\n",
    "            \"cam_in_LWUP\",\n",
    "            \"cam_in_ICEFRAC\",\n",
    "            \"cam_in_LANDFRAC\",\n",
    "            \"cam_in_OCNFRAC\",\n",
    "            \"cam_in_SNOWHICE\",\n",
    "            \"cam_in_SNOWHLAND\",\n",
    "            \"pbuf_ozone\",  # outside of the upper troposphere lower stratosphere (UTLS, corresponding to indices 5-21), variance in minimal for these last 3\n",
    "            \"pbuf_CH4\",\n",
    "            \"pbuf_N2O\",\n",
    "        ]\n",
    "\n",
    "        self.v2_outputs = [\n",
    "            \"ptend_t\",\n",
    "            \"ptend_q0001\",\n",
    "            \"ptend_q0002\",\n",
    "            \"ptend_q0003\",\n",
    "            \"ptend_u\",\n",
    "            \"ptend_v\",\n",
    "            \"cam_out_NETSW\",\n",
    "            \"cam_out_FLWDS\",\n",
    "            \"cam_out_PRECSC\",\n",
    "            \"cam_out_PRECC\",\n",
    "            \"cam_out_SOLS\",\n",
    "            \"cam_out_SOLL\",\n",
    "            \"cam_out_SOLSD\",\n",
    "            \"cam_out_SOLLD\",\n",
    "        ]\n",
    "\n",
    "        self.var_lens = {  # inputs\n",
    "            \"state_t\": self.num_levels,\n",
    "            \"state_q0001\": self.num_levels,\n",
    "            \"state_q0002\": self.num_levels,\n",
    "            \"state_q0003\": self.num_levels,\n",
    "            \"state_u\": self.num_levels,\n",
    "            \"state_v\": self.num_levels,\n",
    "            \"state_ps\": 1,\n",
    "            \"pbuf_SOLIN\": 1,\n",
    "            \"pbuf_LHFLX\": 1,\n",
    "            \"pbuf_SHFLX\": 1,\n",
    "            \"pbuf_TAUX\": 1,\n",
    "            \"pbuf_TAUY\": 1,\n",
    "            \"pbuf_COSZRS\": 1,\n",
    "            \"cam_in_ALDIF\": 1,\n",
    "            \"cam_in_ALDIR\": 1,\n",
    "            \"cam_in_ASDIF\": 1,\n",
    "            \"cam_in_ASDIR\": 1,\n",
    "            \"cam_in_LWUP\": 1,\n",
    "            \"cam_in_ICEFRAC\": 1,\n",
    "            \"cam_in_LANDFRAC\": 1,\n",
    "            \"cam_in_OCNFRAC\": 1,\n",
    "            \"cam_in_SNOWHICE\": 1,\n",
    "            \"cam_in_SNOWHLAND\": 1,\n",
    "            \"pbuf_ozone\": self.num_levels,\n",
    "            \"pbuf_CH4\": self.num_levels,\n",
    "            \"pbuf_N2O\": self.num_levels,\n",
    "            # outputs\n",
    "            \"ptend_t\": self.num_levels,\n",
    "            \"ptend_q0001\": self.num_levels,\n",
    "            \"ptend_q0002\": self.num_levels,\n",
    "            \"ptend_q0003\": self.num_levels,\n",
    "            \"ptend_u\": self.num_levels,\n",
    "            \"ptend_v\": self.num_levels,\n",
    "            \"cam_out_NETSW\": 1,\n",
    "            \"cam_out_FLWDS\": 1,\n",
    "            \"cam_out_PRECSC\": 1,\n",
    "            \"cam_out_PRECC\": 1,\n",
    "            \"cam_out_SOLS\": 1,\n",
    "            \"cam_out_SOLL\": 1,\n",
    "            \"cam_out_SOLSD\": 1,\n",
    "            \"cam_out_SOLLD\": 1,\n",
    "        }\n",
    "\n",
    "        self.var_short_names = {\n",
    "            \"ptend_t\": \"$dT/dt$\",\n",
    "            \"ptend_q0001\": \"$dq/dt$\",\n",
    "            \"cam_out_NETSW\": \"NETSW\",\n",
    "            \"cam_out_FLWDS\": \"FLWDS\",\n",
    "            \"cam_out_PRECSC\": \"PRECSC\",\n",
    "            \"cam_out_PRECC\": \"PRECC\",\n",
    "            \"cam_out_SOLS\": \"SOLS\",\n",
    "            \"cam_out_SOLL\": \"SOLL\",\n",
    "            \"cam_out_SOLSD\": \"SOLSD\",\n",
    "            \"cam_out_SOLLD\": \"SOLLD\",\n",
    "        }\n",
    "\n",
    "        self.target_energy_conv = {\n",
    "            \"ptend_t\": self.cp,\n",
    "            \"ptend_q0001\": self.lv,\n",
    "            \"ptend_q0002\": self.lv,\n",
    "            \"ptend_q0003\": self.lv,\n",
    "            \"ptend_wind\": None,\n",
    "            \"cam_out_NETSW\": 1.0,\n",
    "            \"cam_out_FLWDS\": 1.0,\n",
    "            \"cam_out_PRECSC\": self.lv * self.rho_h20,\n",
    "            \"cam_out_PRECC\": self.lv * self.rho_h20,\n",
    "            \"cam_out_SOLS\": 1.0,\n",
    "            \"cam_out_SOLL\": 1.0,\n",
    "            \"cam_out_SOLSD\": 1.0,\n",
    "            \"cam_out_SOLLD\": 1.0,\n",
    "        }\n",
    "\n",
    "        # for metrics\n",
    "\n",
    "        self.input_train = None\n",
    "        self.target_train = None\n",
    "        self.preds_train = None\n",
    "        self.samplepreds_train = None\n",
    "        self.target_weighted_train = {}\n",
    "        self.preds_weighted_train = {}\n",
    "        self.samplepreds_weighted_train = {}\n",
    "        self.metrics_train = []\n",
    "        self.metrics_idx_train = {}\n",
    "        self.metrics_var_train = {}\n",
    "\n",
    "        self.input_val = None\n",
    "        self.target_val = None\n",
    "        self.preds_val = None\n",
    "        self.samplepreds_val = None\n",
    "        self.target_weighted_val = {}\n",
    "        self.preds_weighted_val = {}\n",
    "        self.samplepreds_weighted_val = {}\n",
    "        self.metrics_val = []\n",
    "        self.metrics_idx_val = {}\n",
    "        self.metrics_var_val = {}\n",
    "\n",
    "        self.input_scoring = None\n",
    "        self.target_scoring = None\n",
    "        self.preds_scoring = None\n",
    "        self.samplepreds_scoring = None\n",
    "        self.target_weighted_scoring = {}\n",
    "        self.preds_weighted_scoring = {}\n",
    "        self.samplepreds_weighted_scoring = {}\n",
    "        self.metrics_scoring = []\n",
    "        self.metrics_idx_scoring = {}\n",
    "        self.metrics_var_scoring = {}\n",
    "\n",
    "        self.input_test = None\n",
    "        self.target_test = None\n",
    "        self.preds_test = None\n",
    "        self.samplepreds_test = None\n",
    "        self.target_weighted_test = {}\n",
    "        self.preds_weighted_test = {}\n",
    "        self.samplepreds_weighted_test = {}\n",
    "        self.metrics_test = []\n",
    "        self.metrics_idx_test = {}\n",
    "        self.metrics_var_test = {}\n",
    "\n",
    "        self.model_names = []\n",
    "        self.metrics_names = []\n",
    "        self.metrics_dict = {\n",
    "            \"MAE\": self.calc_MAE,\n",
    "            \"RMSE\": self.calc_RMSE,\n",
    "            \"R2\": self.calc_R2,\n",
    "            \"CRPS\": self.calc_CRPS,\n",
    "            \"bias\": self.calc_bias,\n",
    "        }\n",
    "        self.num_CRPS = 32\n",
    "        self.linecolors = [\"#0072B2\", \"#E69F00\", \"#882255\", \"#009E73\", \"#D55E00\"]\n",
    "\n",
    "    def set_to_v2_vars(self):\n",
    "        \"\"\"\n",
    "        This function sets the inputs and outputs to the V2 subset.\n",
    "        It also indicates the index of the surface pressure variable.\n",
    "        \"\"\"\n",
    "        self.input_vars = self.v2_inputs\n",
    "        self.target_vars = self.v2_outputs\n",
    "        self.ps_index = 360\n",
    "        self.input_feature_len = 557\n",
    "        self.target_feature_len = 368\n",
    "        self.full_vars = True\n",
    "\n",
    "    def get_xrdata(self, file, file_vars=None):\n",
    "        \"\"\"\n",
    "        This function reads in a file and returns an xarray dataset with the variables specified.\n",
    "        file_vars must be a list of strings.\n",
    "        \"\"\"\n",
    "        ds = xr.open_dataset(file, engine=\"netcdf4\")\n",
    "        if file_vars is not None:\n",
    "            ds = ds[file_vars]\n",
    "        ds = ds.merge(self.grid_info[[\"lat\", \"lon\"]])\n",
    "        ds = ds.where((ds[\"lat\"] > -999) * (ds[\"lat\"] < 999), drop=True)\n",
    "        ds = ds.where((ds[\"lon\"] > -999) * (ds[\"lon\"] < 999), drop=True)\n",
    "        return ds\n",
    "\n",
    "    def get_input(self, input_file):\n",
    "        \"\"\"\n",
    "        This function reads in a file and returns an xarray dataset with the input variables for the emulator.\n",
    "        \"\"\"\n",
    "        # read inputs\n",
    "        return self.get_xrdata(input_file, self.input_vars)\n",
    "\n",
    "    def get_target(self, input_file):\n",
    "        \"\"\"\n",
    "        This function reads in a file and returns an xarray dataset with the target variables for the emulator.\n",
    "        \"\"\"\n",
    "        # read inputs\n",
    "        ds_input = self.get_input(input_file)\n",
    "        ds_target = self.get_xrdata(input_file.replace(\".mli.\", \".mlo.\"))\n",
    "        # each timestep is 20 minutes which corresponds to 1200 seconds\n",
    "        ds_target[\"ptend_t\"] = (\n",
    "            ds_target[\"state_t\"] - ds_input[\"state_t\"]\n",
    "        ) / 1200  # T tendency [K/s]\n",
    "        ds_target[\"ptend_q0001\"] = (\n",
    "            ds_target[\"state_q0001\"] - ds_input[\"state_q0001\"]\n",
    "        ) / 1200  # Q tendency [kg/kg/s]\n",
    "        if self.full_vars:\n",
    "            ds_target[\"ptend_q0002\"] = (\n",
    "                ds_target[\"state_q0002\"] - ds_input[\"state_q0002\"]\n",
    "            ) / 1200  # Q tendency [kg/kg/s]\n",
    "            ds_target[\"ptend_q0003\"] = (\n",
    "                ds_target[\"state_q0003\"] - ds_input[\"state_q0003\"]\n",
    "            ) / 1200  # Q tendency [kg/kg/s]\n",
    "            ds_target[\"ptend_u\"] = (\n",
    "                ds_target[\"state_u\"] - ds_input[\"state_u\"]\n",
    "            ) / 1200  # U tendency [m/s/s]\n",
    "            ds_target[\"ptend_v\"] = (\n",
    "                ds_target[\"state_v\"] - ds_input[\"state_v\"]\n",
    "            ) / 1200  # V tendency [m/s/s]\n",
    "        ds_target = ds_target[self.target_vars]\n",
    "        return ds_target\n",
    "\n",
    "    def set_regexps(self, data_split, regexps):\n",
    "        \"\"\"\n",
    "        This function sets the regular expressions used for getting the filelist for train, val, scoring, and test.\n",
    "        \"\"\"\n",
    "        assert data_split in [\n",
    "            \"train\",\n",
    "            \"val\",\n",
    "            \"scoring\",\n",
    "            \"test\",\n",
    "        ], \"Provided data_split is not valid. Available options are train, val, scoring, and test.\"\n",
    "        if data_split == \"train\":\n",
    "            self.train_regexps = regexps\n",
    "        elif data_split == \"val\":\n",
    "            self.val_regexps = regexps\n",
    "        elif data_split == \"scoring\":\n",
    "            self.scoring_regexps = regexps\n",
    "        elif data_split == \"test\":\n",
    "            self.test_regexps = regexps\n",
    "\n",
    "    def set_stride_sample(self, data_split, stride_sample):\n",
    "        \"\"\"\n",
    "        This function sets the stride_sample for train, val, scoring, and test.\n",
    "        \"\"\"\n",
    "        assert data_split in [\n",
    "            \"train\",\n",
    "            \"val\",\n",
    "            \"scoring\",\n",
    "            \"test\",\n",
    "        ], \"Provided data_split is not valid. Available options are train, val, scoring, and test.\"\n",
    "        if data_split == \"train\":\n",
    "            self.train_stride_sample = stride_sample\n",
    "        elif data_split == \"val\":\n",
    "            self.val_stride_sample = stride_sample\n",
    "        elif data_split == \"scoring\":\n",
    "            self.scoring_stride_sample = stride_sample\n",
    "        elif data_split == \"test\":\n",
    "            self.test_stride_sample = stride_sample\n",
    "\n",
    "    def set_filelist(self, data_split):\n",
    "        \"\"\"\n",
    "        This function sets the filelists corresponding to data splits for train, val, scoring, and test.\n",
    "        \"\"\"\n",
    "        filelist = []\n",
    "        assert data_split in [\n",
    "            \"train\",\n",
    "            \"val\",\n",
    "            \"scoring\",\n",
    "            \"test\",\n",
    "        ], \"Provided data_split is not valid. Available options are train, val, scoring, and test.\"\n",
    "        if data_split == \"train\":\n",
    "            assert self.train_regexps is not None, \"regexps for train is not set.\"\n",
    "            assert (\n",
    "                self.train_stride_sample is not None\n",
    "            ), \"stride_sample for train is not set.\"\n",
    "            for regexp in self.train_regexps:\n",
    "                filelist = filelist + glob.glob(self.data_path + \"*/\" + regexp)\n",
    "            self.train_filelist = sorted(filelist)[:: self.train_stride_sample]\n",
    "        elif data_split == \"val\":\n",
    "            assert self.val_regexps is not None, \"regexps for val is not set.\"\n",
    "            assert (\n",
    "                self.val_stride_sample is not None\n",
    "            ), \"stride_sample for val is not set.\"\n",
    "            for regexp in self.val_regexps:\n",
    "                filelist = filelist + glob.glob(self.data_path + \"*/\" + regexp)\n",
    "            self.val_filelist = sorted(filelist)[:: self.val_stride_sample]\n",
    "        elif data_split == \"scoring\":\n",
    "            assert self.scoring_regexps is not None, \"regexps for scoring is not set.\"\n",
    "            assert (\n",
    "                self.scoring_stride_sample is not None\n",
    "            ), \"stride_sample for scoring is not set.\"\n",
    "            for regexp in self.scoring_regexps:\n",
    "                filelist = filelist + glob.glob(self.data_path + \"*/\" + regexp)\n",
    "            self.scoring_filelist = sorted(filelist)[:: self.scoring_stride_sample]\n",
    "        elif data_split == \"test\":\n",
    "            assert self.test_regexps is not None, \"regexps for test is not set.\"\n",
    "            assert (\n",
    "                self.test_stride_sample is not None\n",
    "            ), \"stride_sample for test is not set.\"\n",
    "            for regexp in self.test_regexps:\n",
    "                filelist = filelist + glob.glob(self.data_path + \"*/\" + regexp)\n",
    "            self.test_filelist = sorted(filelist)[:: self.test_stride_sample]\n",
    "\n",
    "    def get_filelist(self, data_split):\n",
    "        \"\"\"\n",
    "        This function returns the filelist corresponding to data splits for train, val, scoring, and test.\n",
    "        \"\"\"\n",
    "        assert data_split in [\n",
    "            \"train\",\n",
    "            \"val\",\n",
    "            \"scoring\",\n",
    "            \"test\",\n",
    "        ], \"Provided data_split is not valid. Available options are train, val, scoring, and test.\"\n",
    "        if data_split == \"train\":\n",
    "            assert self.train_filelist is not None, \"filelist for train is not set.\"\n",
    "            return self.train_filelist\n",
    "        elif data_split == \"val\":\n",
    "            assert self.val_filelist is not None, \"filelist for val is not set.\"\n",
    "            return self.val_filelist\n",
    "        elif data_split == \"scoring\":\n",
    "            assert self.scoring_filelist is not None, \"filelist for scoring is not set.\"\n",
    "            return self.scoring_filelist\n",
    "        elif data_split == \"test\":\n",
    "            assert self.test_filelist is not None, \"filelist for test is not set.\"\n",
    "            return self.test_filelist\n",
    "\n",
    "    def load_ncdata_with_generator(self, data_split):\n",
    "        \"\"\"\n",
    "        This function works as a dataloader when training the emulator with raw netCDF files.\n",
    "        This can be used as a dataloader during training or it can be used to create entire datasets.\n",
    "        When used as a dataloader for training, I/O can slow down training considerably.\n",
    "        This function also normalizes the data.\n",
    "        mli corresponds to input\n",
    "        mlo corresponds to target\n",
    "        \"\"\"\n",
    "        filelist = self.get_filelist(data_split)\n",
    "\n",
    "        def gen():\n",
    "            for file in filelist:\n",
    "                # read inputs\n",
    "                ds_input = self.get_input(file)\n",
    "                # read targets\n",
    "                ds_target = self.get_target(file)\n",
    "\n",
    "                # normalization, scaling\n",
    "                if self.normalize:\n",
    "                    ds_input = (ds_input - self.input_mean) / (\n",
    "                        self.input_max - self.input_min\n",
    "                    )\n",
    "                    ds_target = ds_target * self.output_scale\n",
    "                else:\n",
    "                    ds_input = ds_input.drop([\"lat\", \"lon\"])\n",
    "\n",
    "                # stack\n",
    "                # ds = ds.stack({'batch':{'sample','ncol'}})\n",
    "                ds_input = ds_input.stack({\"batch\": {\"ncol\"}})\n",
    "                ds_input = ds_input.to_stacked_array(\n",
    "                    \"mlvar\", sample_dims=[\"batch\"], name=\"mli\"\n",
    "                )\n",
    "                # dso = dso.stack({'batch':{'sample','ncol'}})\n",
    "                ds_target = ds_target.stack({\"batch\": {\"ncol\"}})\n",
    "                ds_target = ds_target.to_stacked_array(\n",
    "                    \"mlvar\", sample_dims=[\"batch\"], name=\"mlo\"\n",
    "                )\n",
    "                yield (ds_input.values, ds_target.values)\n",
    "\n",
    "        if self.ml_backend == \"tensorflow\":\n",
    "\n",
    "            # Removed output_shapes and output_types, converting to output_signature as is\n",
    "            # recommended in the latest version of TensorFlow.\n",
    "            return self.tf.data.Dataset.from_generator(\n",
    "                gen,\n",
    "                output_signature=(\n",
    "                    self.tf.TensorSpec(\n",
    "                        shape=(None, self.input_feature_len), dtype=self.tf.float64\n",
    "                    ),\n",
    "                    self.tf.TensorSpec(\n",
    "                        shape=(None, self.target_feature_len), dtype=self.tf.float64\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        elif self.ml_backend == \"pytorch\":\n",
    "            if self.successful_backend_import:\n",
    "\n",
    "                class IterableTorchDataset(self.torch.utils.data.IterableDataset):\n",
    "                    def __init__(\n",
    "                        this_self, data_generator, output_types, output_shapes\n",
    "                    ):\n",
    "                        this_self.data_generator = data_generator\n",
    "                        this_self.output_types = output_types\n",
    "                        this_self.output_shapes = output_shapes\n",
    "\n",
    "                    def __iter__(this_self):\n",
    "                        for item in this_self.data_generator:\n",
    "\n",
    "                            input_array = self.torch.tensor(\n",
    "                                item[0], dtype=this_self.output_types[0]\n",
    "                            )\n",
    "                            target_array = self.torch.tensor(\n",
    "                                item[1], dtype=this_self.output_types[1]\n",
    "                            )\n",
    "\n",
    "                            # Assert final dimensions are correct.\n",
    "                            assert (\n",
    "                                input_array.shape[-1] == this_self.output_shapes[0][-1]\n",
    "                            )\n",
    "                            assert (\n",
    "                                target_array.shape[-1] == this_self.output_shapes[1][-1]\n",
    "                            )\n",
    "\n",
    "                            yield (input_array, target_array)\n",
    "\n",
    "                    def as_numpy_iterator(this_self):\n",
    "                        for item in this_self.data_generator:\n",
    "\n",
    "                            # Convert item to numpy array\n",
    "                            input_array = np.array(item[0])\n",
    "                            target_array = np.array(item[1])\n",
    "\n",
    "                            # Assert final dimensions are correct.\n",
    "                            assert (\n",
    "                                input_array.shape[-1] == this_self.output_shapes[0][-1]\n",
    "                            )\n",
    "                            assert (\n",
    "                                target_array.shape[-1] == this_self.output_shapes[1][-1]\n",
    "                            )\n",
    "\n",
    "                            yield (input_array, target_array)\n",
    "\n",
    "                dataset = IterableTorchDataset(\n",
    "                    gen(),\n",
    "                    (self.torch.float64, self.torch.float64),\n",
    "                    ((None, self.input_feature_len), (None, self.target_feature_len)),\n",
    "                )\n",
    "\n",
    "                return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def ls(dir_path=\"\"):\n",
    "        \"\"\"\n",
    "        You can treat this as a Python wrapper for the bash command \"ls\".\n",
    "        \"\"\"\n",
    "        return os.popen(\" \".join([\"ls\", dir_path])).read().splitlines()\n",
    "\n",
    "    @staticmethod\n",
    "    def set_plot_params():\n",
    "        \"\"\"\n",
    "        This function sets the plot parameters for matplotlib.\n",
    "        \"\"\"\n",
    "        plt.close(\"all\")\n",
    "        plt.rcParams.update(plt.rcParamsDefault)\n",
    "        plt.rc(\"font\", family=\"sans\")\n",
    "        plt.rcParams.update(\n",
    "            {\n",
    "                \"font.size\": 32,\n",
    "                \"lines.linewidth\": 2,\n",
    "                \"axes.labelsize\": 32,\n",
    "                \"axes.titlesize\": 32,\n",
    "                \"xtick.labelsize\": 32,\n",
    "                \"ytick.labelsize\": 32,\n",
    "                \"legend.fontsize\": 32,\n",
    "                \"axes.linewidth\": 2,\n",
    "                \"pgf.texsystem\": \"pdflatex\",\n",
    "            }\n",
    "        )\n",
    "        # %config InlineBackend.figure_format = 'retina'\n",
    "        # use the above line when working in a jupyter notebook\n",
    "\n",
    "    @staticmethod\n",
    "    def load_npy_file(load_path=\"\"):\n",
    "        \"\"\"\n",
    "        This function loads the prediction .npy file.\n",
    "        \"\"\"\n",
    "        with open(load_path, \"rb\") as f:\n",
    "            pred = np.load(f)\n",
    "        return pred\n",
    "\n",
    "    @staticmethod\n",
    "    def load_h5_file(load_path=\"\"):\n",
    "        \"\"\"\n",
    "        This function loads the prediction .h5 file.\n",
    "        \"\"\"\n",
    "        hf = h5py.File(load_path, \"r\")\n",
    "        pred = np.array(hf.get(\"pred\"))\n",
    "        return pred\n",
    "\n",
    "    def output_weighting(self, output, data_split, just_weights=False):\n",
    "        \"\"\"\n",
    "        This function does four transformations, and assumes we are using V1 variables:\n",
    "        [0] Undos the output scaling\n",
    "        [1] Weight vertical levels by dp/g\n",
    "        [2] Weight horizontal area of each grid cell by a[x]/mean(a[x])\n",
    "        [3] Unit conversion to a common energy unit\n",
    "        \"\"\"\n",
    "        assert data_split in [\n",
    "            \"train\",\n",
    "            \"val\",\n",
    "            \"scoring\",\n",
    "            \"test\",\n",
    "        ], \"Provided data_split is not valid. Available options are train, val, scoring, and test.\"\n",
    "        num_samples = output.shape[0]\n",
    "        if just_weights:\n",
    "            weightings = np.ones(output.shape)\n",
    "\n",
    "        if not self.full_vars:\n",
    "            ptend_t = output[:, :60].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "            )\n",
    "            ptend_q0001 = output[:, 60:120].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "            )\n",
    "            netsw = output[:, 120].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            flwds = output[:, 121].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            precsc = output[:, 122].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            precc = output[:, 123].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            sols = output[:, 124].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            soll = output[:, 125].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            solsd = output[:, 126].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            solld = output[:, 127].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            if just_weights:\n",
    "                ptend_t_weight = weightings[:, :60].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "                )\n",
    "                ptend_q0001_weight = weightings[:, 60:120].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "                )\n",
    "                netsw_weight = weightings[:, 120].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                flwds_weight = weightings[:, 121].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                precsc_weight = weightings[:, 122].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                precc_weight = weightings[:, 123].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                sols_weight = weightings[:, 124].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                soll_weight = weightings[:, 125].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                solsd_weight = weightings[:, 126].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                solld_weight = weightings[:, 127].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "        else:\n",
    "            ptend_t = output[:, :60].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "            )\n",
    "            ptend_q0001 = output[:, 60:120].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "            )\n",
    "            ptend_q0002 = output[:, 120:180].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "            )\n",
    "            ptend_q0003 = output[:, 180:240].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "            )\n",
    "            ptend_u = output[:, 240:300].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "            )\n",
    "            ptend_v = output[:, 300:360].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "            )\n",
    "            netsw = output[:, 360].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            flwds = output[:, 361].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            precsc = output[:, 362].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            precc = output[:, 363].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            sols = output[:, 364].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            soll = output[:, 365].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            solsd = output[:, 366].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            solld = output[:, 367].reshape(\n",
    "                (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "            )\n",
    "            state_wind = ((ptend_u**2) + (ptend_v**2)) ** 0.5\n",
    "            self.target_energy_conv[\"ptend_wind\"] = state_wind\n",
    "            if just_weights:\n",
    "                ptend_t_weight = weightings[:, :60].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "                )\n",
    "                ptend_q0001_weight = weightings[:, 60:120].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "                )\n",
    "                ptend_q0002_weight = weightings[:, 120:180].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "                )\n",
    "                ptend_q0003_weight = weightings[:, 180:240].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "                )\n",
    "                ptend_u_weight = weightings[:, 240:300].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "                )\n",
    "                ptend_v_weight = weightings[:, 300:360].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "                )\n",
    "                netsw_weight = weightings[:, 360].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                flwds_weight = weightings[:, 361].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                precsc_weight = weightings[:, 362].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                precc_weight = weightings[:, 363].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                sols_weight = weightings[:, 364].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                soll_weight = weightings[:, 365].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                solsd_weight = weightings[:, 366].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "                solld_weight = weightings[:, 367].reshape(\n",
    "                    (int(num_samples / self.num_latlon), self.num_latlon)\n",
    "                )\n",
    "\n",
    "        # ptend_t = ptend_t.transpose((2,0,1))\n",
    "        # ptend_q0001 = ptend_q0001.transpose((2,0,1))\n",
    "        # scalar_outputs = scalar_outputs.transpose((2,0,1))\n",
    "\n",
    "        # [0] Undo output scaling\n",
    "        if self.normalize:\n",
    "            ptend_t = (\n",
    "                ptend_t / self.output_scale[\"ptend_t\"].values[np.newaxis, np.newaxis, :]\n",
    "            )\n",
    "            ptend_q0001 = (\n",
    "                ptend_q0001\n",
    "                / self.output_scale[\"ptend_q0001\"].values[np.newaxis, np.newaxis, :]\n",
    "            )\n",
    "            netsw = netsw / self.output_scale[\"cam_out_NETSW\"].values\n",
    "            flwds = flwds / self.output_scale[\"cam_out_FLWDS\"].values\n",
    "            precsc = precsc / self.output_scale[\"cam_out_PRECSC\"].values\n",
    "            precc = precc / self.output_scale[\"cam_out_PRECC\"].values\n",
    "            sols = sols / self.output_scale[\"cam_out_SOLS\"].values\n",
    "            soll = soll / self.output_scale[\"cam_out_SOLL\"].values\n",
    "            solsd = solsd / self.output_scale[\"cam_out_SOLSD\"].values\n",
    "            solld = solld / self.output_scale[\"cam_out_SOLLD\"].values\n",
    "            if just_weights:\n",
    "                ptend_t_weight = (\n",
    "                    ptend_t_weight\n",
    "                    / self.output_scale[\"ptend_t\"].values[np.newaxis, np.newaxis, :]\n",
    "                )\n",
    "                ptend_q0001_weight = (\n",
    "                    ptend_q0001_weight\n",
    "                    / self.output_scale[\"ptend_q0001\"].values[np.newaxis, np.newaxis, :]\n",
    "                )\n",
    "                netsw_weight = netsw_weight / self.output_scale[\"cam_out_NETSW\"].values\n",
    "                flwds_weight = flwds_weight / self.output_scale[\"cam_out_FLWDS\"].values\n",
    "                precsc_weight = (\n",
    "                    precsc_weight / self.output_scale[\"cam_out_PRECSC\"].values\n",
    "                )\n",
    "                precc_weight = precc_weight / self.output_scale[\"cam_out_PRECC\"].values\n",
    "                sols_weight = sols_weight / self.output_scale[\"cam_out_SOLS\"].values\n",
    "                soll_weight = soll_weight / self.output_scale[\"cam_out_SOLL\"].values\n",
    "                solsd_weight = solsd_weight / self.output_scale[\"cam_out_SOLSD\"].values\n",
    "                solld_weight = solld_weight / self.output_scale[\"cam_out_SOLLD\"].values\n",
    "            if self.full_vars:\n",
    "                ptend_q0002 = (\n",
    "                    ptend_q0002\n",
    "                    / self.output_scale[\"ptend_q0002\"].values[np.newaxis, np.newaxis, :]\n",
    "                )\n",
    "                ptend_q0003 = (\n",
    "                    ptend_q0003\n",
    "                    / self.output_scale[\"ptend_q0003\"].values[np.newaxis, np.newaxis, :]\n",
    "                )\n",
    "                ptend_u = (\n",
    "                    ptend_u\n",
    "                    / self.output_scale[\"ptend_u\"].values[np.newaxis, np.newaxis, :]\n",
    "                )\n",
    "                ptend_v = (\n",
    "                    ptend_v\n",
    "                    / self.output_scale[\"ptend_v\"].values[np.newaxis, np.newaxis, :]\n",
    "                )\n",
    "                if just_weights:\n",
    "                    ptend_q0002_weight = (\n",
    "                        ptend_q0002_weight\n",
    "                        / self.output_scale[\"ptend_q0002\"].values[\n",
    "                            np.newaxis, np.newaxis, :\n",
    "                        ]\n",
    "                    )\n",
    "                    ptend_q0003_weight = (\n",
    "                        ptend_q0003_weight\n",
    "                        / self.output_scale[\"ptend_q0003\"].values[\n",
    "                            np.newaxis, np.newaxis, :\n",
    "                        ]\n",
    "                    )\n",
    "                    ptend_u_weight = (\n",
    "                        ptend_u_weight\n",
    "                        / self.output_scale[\"ptend_u\"].values[np.newaxis, np.newaxis, :]\n",
    "                    )\n",
    "                    ptend_v_weight = (\n",
    "                        ptend_v_weight\n",
    "                        / self.output_scale[\"ptend_v\"].values[np.newaxis, np.newaxis, :]\n",
    "                    )\n",
    "\n",
    "        # [1] Weight vertical levels by dp/g\n",
    "        # only for vertically-resolved variables, e.g. ptend_{t,q0001}\n",
    "        # dp/g = -\\rho * dz\n",
    "\n",
    "        dp = None\n",
    "        if data_split == \"train\":\n",
    "            dp = self.dp_train\n",
    "        elif data_split == \"val\":\n",
    "            dp = self.dp_val\n",
    "        elif data_split == \"scoring\":\n",
    "            dp = self.dp_scoring\n",
    "        elif data_split == \"test\":\n",
    "            dp = self.dp_test\n",
    "        assert dp is not None\n",
    "        ptend_t = ptend_t * dp / self.grav\n",
    "        ptend_q0001 = ptend_q0001 * dp / self.grav\n",
    "        if just_weights:\n",
    "            ptend_t_weight = ptend_t_weight * dp / self.grav\n",
    "            ptend_q0001_weight = ptend_q0001_weight * dp / self.grav\n",
    "        if self.full_vars:\n",
    "            ptend_q0002 = ptend_q0002 * dp / self.grav\n",
    "            ptend_q0003 = ptend_q0003 * dp / self.grav\n",
    "            ptend_u = ptend_u * dp / self.grav\n",
    "            ptend_v = ptend_v * dp / self.grav\n",
    "            if just_weights:\n",
    "                ptend_q0002_weight = ptend_q0002_weight * dp / self.grav\n",
    "                ptend_q0003_weight = ptend_q0003_weight * dp / self.grav\n",
    "                ptend_u_weight = ptend_u_weight * dp / self.grav\n",
    "                ptend_v_weight = ptend_v_weight * dp / self.grav\n",
    "\n",
    "        # [2] weight by area\n",
    "\n",
    "        ptend_t = ptend_t * self.area_wgt[np.newaxis, :, np.newaxis]\n",
    "        ptend_q0001 = ptend_q0001 * self.area_wgt[np.newaxis, :, np.newaxis]\n",
    "        netsw = netsw * self.area_wgt[np.newaxis, :]\n",
    "        flwds = flwds * self.area_wgt[np.newaxis, :]\n",
    "        precsc = precsc * self.area_wgt[np.newaxis, :]\n",
    "        precc = precc * self.area_wgt[np.newaxis, :]\n",
    "        sols = sols * self.area_wgt[np.newaxis, :]\n",
    "        soll = soll * self.area_wgt[np.newaxis, :]\n",
    "        solsd = solsd * self.area_wgt[np.newaxis, :]\n",
    "        solld = solld * self.area_wgt[np.newaxis, :]\n",
    "        if just_weights:\n",
    "            ptend_t_weight = ptend_t_weight * self.area_wgt[np.newaxis, :, np.newaxis]\n",
    "            ptend_q0001_weight = (\n",
    "                ptend_q0001_weight * self.area_wgt[np.newaxis, :, np.newaxis]\n",
    "            )\n",
    "            netsw_weight = netsw_weight * self.area_wgt[np.newaxis, :]\n",
    "            flwds_weight = flwds_weight * self.area_wgt[np.newaxis, :]\n",
    "            precsc_weight = precsc_weight * self.area_wgt[np.newaxis, :]\n",
    "            precc_weight = precc_weight * self.area_wgt[np.newaxis, :]\n",
    "            sols_weight = sols_weight * self.area_wgt[np.newaxis, :]\n",
    "            soll_weight = soll_weight * self.area_wgt[np.newaxis, :]\n",
    "            solsd_weight = solsd_weight * self.area_wgt[np.newaxis, :]\n",
    "            solld_weight = solld_weight * self.area_wgt[np.newaxis, :]\n",
    "        if self.full_vars:\n",
    "            ptend_q0002 = ptend_q0002 * self.area_wgt[np.newaxis, :, np.newaxis]\n",
    "            ptend_q0003 = ptend_q0003 * self.area_wgt[np.newaxis, :, np.newaxis]\n",
    "            ptend_u = ptend_u * self.area_wgt[np.newaxis, :, np.newaxis]\n",
    "            ptend_v = ptend_v * self.area_wgt[np.newaxis, :, np.newaxis]\n",
    "            if just_weights:\n",
    "                ptend_q0002_weight = (\n",
    "                    ptend_q0002_weight * self.area_wgt[np.newaxis, :, np.newaxis]\n",
    "                )\n",
    "                ptend_q0003_weight = (\n",
    "                    ptend_q0003_weight * self.area_wgt[np.newaxis, :, np.newaxis]\n",
    "                )\n",
    "                ptend_u_weight = (\n",
    "                    ptend_u_weight * self.area_wgt[np.newaxis, :, np.newaxis]\n",
    "                )\n",
    "                ptend_v_weight = (\n",
    "                    ptend_v_weight * self.area_wgt[np.newaxis, :, np.newaxis]\n",
    "                )\n",
    "\n",
    "        # [3] unit conversion\n",
    "\n",
    "        ptend_t = ptend_t * self.target_energy_conv[\"ptend_t\"]\n",
    "        ptend_q0001 = ptend_q0001 * self.target_energy_conv[\"ptend_q0001\"]\n",
    "        netsw = netsw * self.target_energy_conv[\"cam_out_NETSW\"]\n",
    "        flwds = flwds * self.target_energy_conv[\"cam_out_FLWDS\"]\n",
    "        precsc = precsc * self.target_energy_conv[\"cam_out_PRECSC\"]\n",
    "        precc = precc * self.target_energy_conv[\"cam_out_PRECC\"]\n",
    "        sols = sols * self.target_energy_conv[\"cam_out_SOLS\"]\n",
    "        soll = soll * self.target_energy_conv[\"cam_out_SOLL\"]\n",
    "        solsd = solsd * self.target_energy_conv[\"cam_out_SOLSD\"]\n",
    "        solld = solld * self.target_energy_conv[\"cam_out_SOLLD\"]\n",
    "        if just_weights:\n",
    "            ptend_t_weight = ptend_t_weight * self.target_energy_conv[\"ptend_t\"]\n",
    "            ptend_q0001_weight = (\n",
    "                ptend_q0001_weight * self.target_energy_conv[\"ptend_q0001\"]\n",
    "            )\n",
    "            netsw_weight = netsw_weight * self.target_energy_conv[\"cam_out_NETSW\"]\n",
    "            flwds_weight = flwds_weight * self.target_energy_conv[\"cam_out_FLWDS\"]\n",
    "            precsc_weight = precsc_weight * self.target_energy_conv[\"cam_out_PRECSC\"]\n",
    "            precc_weight = precc_weight * self.target_energy_conv[\"cam_out_PRECC\"]\n",
    "            sols_weight = sols_weight * self.target_energy_conv[\"cam_out_SOLS\"]\n",
    "            soll_weight = soll_weight * self.target_energy_conv[\"cam_out_SOLL\"]\n",
    "            solsd_weight = solsd_weight * self.target_energy_conv[\"cam_out_SOLSD\"]\n",
    "            solld_weight = solld_weight * self.target_energy_conv[\"cam_out_SOLLD\"]\n",
    "        if self.full_vars:\n",
    "            ptend_q0002 = ptend_q0002 * self.target_energy_conv[\"ptend_q0002\"]\n",
    "            ptend_q0003 = ptend_q0003 * self.target_energy_conv[\"ptend_q0003\"]\n",
    "            ptend_u = ptend_u * self.target_energy_conv[\"ptend_wind\"]\n",
    "            ptend_v = ptend_v * self.target_energy_conv[\"ptend_wind\"]\n",
    "            if just_weights:\n",
    "                ptend_q0002_weight = (\n",
    "                    ptend_q0002_weight * self.target_energy_conv[\"ptend_q0002\"]\n",
    "                )\n",
    "                ptend_q0003_weight = (\n",
    "                    ptend_q0003_weight * self.target_energy_conv[\"ptend_q0003\"]\n",
    "                )\n",
    "                ptend_u_weight = ptend_u_weight * self.target_energy_conv[\"ptend_wind\"]\n",
    "                ptend_v_weight = ptend_v_weight * self.target_energy_conv[\"ptend_wind\"]\n",
    "\n",
    "        if just_weights:\n",
    "            if self.full_vars:\n",
    "                weightings = np.concatenate(\n",
    "                    [\n",
    "                        ptend_t_weight.reshape((num_samples, 60)),\n",
    "                        ptend_q0001_weight.reshape((num_samples, 60)),\n",
    "                        ptend_q0002_weight.reshape((num_samples, 60)),\n",
    "                        ptend_q0003_weight.reshape((num_samples, 60)),\n",
    "                        ptend_u_weight.reshape((num_samples, 60)),\n",
    "                        ptend_v_weight.reshape((num_samples, 60)),\n",
    "                        netsw_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        flwds_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        precsc_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        precc_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        sols_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        soll_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        solsd_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        solld_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "            else:\n",
    "                weightings = np.concatenate(\n",
    "                    [\n",
    "                        ptend_t_weight.reshape((num_samples, 60)),\n",
    "                        ptend_q0001_weight.reshape((num_samples, 60)),\n",
    "                        netsw_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        flwds_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        precsc_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        precc_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        sols_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        soll_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        solsd_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                        solld_weight.reshape((num_samples))[:, np.newaxis],\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "            return weightings\n",
    "        else:\n",
    "            var_dict = {\n",
    "                \"ptend_t\": ptend_t,\n",
    "                \"ptend_q0001\": ptend_q0001,\n",
    "                \"cam_out_NETSW\": netsw,\n",
    "                \"cam_out_FLWDS\": flwds,\n",
    "                \"cam_out_PRECSC\": precsc,\n",
    "                \"cam_out_PRECC\": precc,\n",
    "                \"cam_out_SOLS\": sols,\n",
    "                \"cam_out_SOLL\": soll,\n",
    "                \"cam_out_SOLSD\": solsd,\n",
    "                \"cam_out_SOLLD\": solld,\n",
    "            }\n",
    "            if self.full_vars:\n",
    "                var_dict[\"ptend_q0002\"] = ptend_q0002\n",
    "                var_dict[\"ptend_q0003\"] = ptend_q0003\n",
    "                var_dict[\"ptend_u\"] = ptend_u\n",
    "                var_dict[\"ptend_v\"] = ptend_v\n",
    "\n",
    "            return var_dict\n",
    "\n",
    "    def calc_MAE(self, pred, target, avg_grid=True):\n",
    "        \"\"\"\n",
    "        calculate 'globally averaged' mean absolute error\n",
    "        for vertically-resolved variables, shape should be time x grid x level\n",
    "        for scalars, shape should be time x grid\n",
    "\n",
    "        returns vector of length level or 1\n",
    "        \"\"\"\n",
    "        assert pred.shape[1] == self.num_latlon\n",
    "        assert pred.shape == target.shape\n",
    "        mae = np.abs(pred - target).mean(axis=0)\n",
    "        if avg_grid:\n",
    "            return mae.mean(axis=0)  # we decided to average globally at end\n",
    "        else:\n",
    "            return mae\n",
    "\n",
    "    def calc_RMSE(self, pred, target, avg_grid=True):\n",
    "        \"\"\"\n",
    "        calculate 'globally averaged' root mean squared error\n",
    "        for vertically-resolved variables, shape should be time x grid x level\n",
    "        for scalars, shape should be time x grid\n",
    "\n",
    "        returns vector of length level or 1\n",
    "        \"\"\"\n",
    "        assert pred.shape[1] == self.num_latlon\n",
    "        assert pred.shape == target.shape\n",
    "        sq_diff = (pred - target) ** 2\n",
    "        rmse = np.sqrt(sq_diff.mean(axis=0))  # mean over time\n",
    "        if avg_grid:\n",
    "            return rmse.mean(axis=0)  # we decided to separately average globally at end\n",
    "        else:\n",
    "            return rmse\n",
    "\n",
    "    def calc_R2(self, pred, target, avg_grid=True):\n",
    "        \"\"\"\n",
    "        calculate 'globally averaged' R-squared\n",
    "        for vertically-resolved variables, input shape should be time x grid x level\n",
    "        for scalars, input shape should be time x grid\n",
    "\n",
    "        returns vector of length level or 1\n",
    "        \"\"\"\n",
    "        assert pred.shape[1] == self.num_latlon\n",
    "        assert pred.shape == target.shape\n",
    "        sq_diff = (pred - target) ** 2\n",
    "        tss_time = (\n",
    "            target - target.mean(axis=0)[np.newaxis, ...]\n",
    "        ) ** 2  # mean over time\n",
    "        r_squared = 1 - sq_diff.sum(axis=0) / tss_time.sum(axis=0)  # sum over time\n",
    "        if avg_grid:\n",
    "            return r_squared.mean(\n",
    "                axis=0\n",
    "            )  # we decided to separately average globally at end\n",
    "        else:\n",
    "            return r_squared\n",
    "\n",
    "    def calc_bias(self, pred, target, avg_grid=True):\n",
    "        \"\"\"\n",
    "        calculate bias\n",
    "        for vertically-resolved variables, input shape should be time x grid x level\n",
    "        for scalars, input shape should be time x grid\n",
    "\n",
    "        returns vector of length level or 1\n",
    "        \"\"\"\n",
    "        assert pred.shape[1] == self.num_latlon\n",
    "        assert pred.shape == target.shape\n",
    "        bias = pred.mean(axis=0) - target.mean(axis=0)\n",
    "        if avg_grid:\n",
    "            return bias.mean(axis=0)  # we decided to separately average globally at end\n",
    "        else:\n",
    "            return bias\n",
    "\n",
    "    def calc_CRPS(self, samplepreds, target, avg_grid=True):\n",
    "        \"\"\"\n",
    "        calculate 'globally averaged' continuous ranked probability score\n",
    "        for vertically-resolved variables, input shape should be time x grid x level x num_crps_samples\n",
    "        for scalars, input shape should be time x grid x num_crps_samples\n",
    "\n",
    "        returns vector of length level or 1\n",
    "        \"\"\"\n",
    "        assert samplepreds.shape[1] == self.num_latlon\n",
    "        assert len(samplepreds.shape) == len(target.shape) + 1\n",
    "        assert len(samplepreds.shape) == 3 or len(samplepreds.shape) == 4\n",
    "        num_crps = samplepreds.shape[-1]\n",
    "        mae = np.mean(\n",
    "            np.abs(samplepreds - target[..., np.newaxis]), axis=(0, -1)\n",
    "        )  # mean over time and crps samples\n",
    "        samplepreds = np.sort(samplepreds, axis=-1)\n",
    "        diff = samplepreds[..., 1:] - samplepreds[..., :-1]\n",
    "        count = np.arange(1, num_crps) * np.arange(num_crps - 1, 0, -1)\n",
    "        if len(samplepreds.shape) == 4:\n",
    "            spread = (\n",
    "                (diff * count[np.newaxis, np.newaxis, np.newaxis, :])\n",
    "                .sum(axis=-1)\n",
    "                .mean(axis=0)\n",
    "            )  # sum over crps samples and mean over time\n",
    "        elif len(samplepreds.shape) == 3:\n",
    "            spread = (\n",
    "                (diff * count[np.newaxis, np.newaxis, :]).sum(axis=-1).mean(axis=0)\n",
    "            )  # sum over crps samples and mean over time\n",
    "        crps = mae - spread / (num_crps * (num_crps - 1))\n",
    "        # count was not multiplied by two so no need to divide by two\n",
    "        if avg_grid:\n",
    "            return crps.mean(axis=0)  # we decided to separately average globally at end\n",
    "        else:\n",
    "            return crps\n",
    "\n",
    "    def reshape_daily(self, output):\n",
    "        \"\"\"\n",
    "        This function returns two numpy arrays, one for each vertically resolved variable (ptend_t and ptend_q0001).\n",
    "        Dimensions of expected input are num_samples by 128 (number of target features).\n",
    "        Output argument is espected to be have dimensions of num_samples by features.\n",
    "        ptend_t is expected to be the first feature, and ptend_q0001 is expected to be the second feature.\n",
    "        Data is expected to use a stride_sample of 6. (12 samples per day, 20 min timestep).\n",
    "        \"\"\"\n",
    "        num_samples = output.shape[0]\n",
    "        ptend_t = output[:, :60].reshape(\n",
    "            (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "        )\n",
    "        ptend_q0001 = output[:, 60:120].reshape(\n",
    "            (int(num_samples / self.num_latlon), self.num_latlon, 60)\n",
    "        )\n",
    "        ptend_t_daily = np.mean(\n",
    "            ptend_t.reshape((ptend_t.shape[0] // 12, 12, self.num_latlon, 60)), axis=1\n",
    "        )  # Nday x lotlonnum x 60\n",
    "        ptend_q0001_daily = np.mean(\n",
    "            ptend_q0001.reshape((ptend_q0001.shape[0] // 12, 12, self.num_latlon, 60)),\n",
    "            axis=1,\n",
    "        )  # Nday x lotlonnum x 60\n",
    "        ptend_t_daily_long = []\n",
    "        ptend_q0001_daily_long = []\n",
    "        for i in range(len(self.lats)):\n",
    "            ptend_t_daily_long.append(\n",
    "                np.mean(ptend_t_daily[:, self.lat_indices_list[i], :], axis=1)\n",
    "            )\n",
    "            ptend_q0001_daily_long.append(\n",
    "                np.mean(ptend_q0001_daily[:, self.lat_indices_list[i], :], axis=1)\n",
    "            )\n",
    "        ptend_t_daily_long = np.array(ptend_t_daily_long)  # lat x Nday x 60\n",
    "        ptend_q0001_daily_long = np.array(ptend_q0001_daily_long)  # lat x Nday x 60\n",
    "        return ptend_t_daily_long, ptend_q0001_daily_long\n",
    "\n",
    "    @staticmethod\n",
    "    def reshape_input_for_cnn(npy_input, save_path=\"\"):\n",
    "        \"\"\"\n",
    "        This function reshapes a numpy input array to be compatible with CNN training.\n",
    "        Each variable becomes its own channel.\n",
    "        For the input there are 6 channels, each with 60 vertical levels.\n",
    "        The last 4 channels correspond to scalars repeated across all 60 levels.\n",
    "        This is for V1 data only! (V2 data has more variables)\n",
    "        \"\"\"\n",
    "        npy_input_cnn = np.stack(\n",
    "            [\n",
    "                npy_input[:, 0:60],\n",
    "                npy_input[:, 60:120],\n",
    "                np.repeat(npy_input[:, 120][:, np.newaxis], 60, axis=1),\n",
    "                np.repeat(npy_input[:, 121][:, np.newaxis], 60, axis=1),\n",
    "                np.repeat(npy_input[:, 122][:, np.newaxis], 60, axis=1),\n",
    "                np.repeat(npy_input[:, 123][:, np.newaxis], 60, axis=1),\n",
    "            ],\n",
    "            axis=2,\n",
    "        )\n",
    "\n",
    "        if save_path != \"\":\n",
    "            with open(save_path + \"train_input_cnn.npy\", \"wb\") as f:\n",
    "                np.save(f, np.float32(npy_input_cnn))\n",
    "        return npy_input_cnn\n",
    "\n",
    "    @staticmethod\n",
    "    def reshape_target_for_cnn(npy_target, save_path=\"\"):\n",
    "        \"\"\"\n",
    "        This function reshapes a numpy target array to be compatible with CNN training.\n",
    "        Each variable becomes its own channel.\n",
    "        For the input there are 6 channels, each with 60 vertical levels.\n",
    "        The last 4 channels correspond to scalars repeated across all 60 levels.\n",
    "        This is for V1 data only! (V2 data has more variables)\n",
    "        \"\"\"\n",
    "        npy_target_cnn = np.stack(\n",
    "            [\n",
    "                npy_target[:, 0:60],\n",
    "                npy_target[:, 60:120],\n",
    "                np.repeat(npy_target[:, 120][:, np.newaxis], 60, axis=1),\n",
    "                np.repeat(npy_target[:, 121][:, np.newaxis], 60, axis=1),\n",
    "                np.repeat(npy_target[:, 122][:, np.newaxis], 60, axis=1),\n",
    "                np.repeat(npy_target[:, 123][:, np.newaxis], 60, axis=1),\n",
    "                np.repeat(npy_target[:, 124][:, np.newaxis], 60, axis=1),\n",
    "                np.repeat(npy_target[:, 125][:, np.newaxis], 60, axis=1),\n",
    "                np.repeat(npy_target[:, 126][:, np.newaxis], 60, axis=1),\n",
    "                np.repeat(npy_target[:, 127][:, np.newaxis], 60, axis=1),\n",
    "            ],\n",
    "            axis=2,\n",
    "        )\n",
    "\n",
    "        if save_path != \"\":\n",
    "            with open(save_path + \"train_target_cnn.npy\", \"wb\") as f:\n",
    "                np.save(f, np.float32(npy_target_cnn))\n",
    "        return npy_target_cnn\n",
    "\n",
    "    @staticmethod\n",
    "    def reshape_target_from_cnn(npy_predict_cnn, save_path=\"\"):\n",
    "        \"\"\"\n",
    "        This function reshapes CNN target to (num_samples, 128) for standardized metrics.\n",
    "        This is for V1 data only! (V2 data has more variables)\n",
    "        \"\"\"\n",
    "        npy_predict_cnn_reshaped = np.concatenate(\n",
    "            [\n",
    "                npy_predict_cnn[:, :, 0],\n",
    "                npy_predict_cnn[:, :, 1],\n",
    "                np.mean(npy_predict_cnn[:, :, 2], axis=1)[:, np.newaxis],\n",
    "                np.mean(npy_predict_cnn[:, :, 3], axis=1)[:, np.newaxis],\n",
    "                np.mean(npy_predict_cnn[:, :, 4], axis=1)[:, np.newaxis],\n",
    "                np.mean(npy_predict_cnn[:, :, 5], axis=1)[:, np.newaxis],\n",
    "                np.mean(npy_predict_cnn[:, :, 6], axis=1)[:, np.newaxis],\n",
    "                np.mean(npy_predict_cnn[:, :, 7], axis=1)[:, np.newaxis],\n",
    "                np.mean(npy_predict_cnn[:, :, 8], axis=1)[:, np.newaxis],\n",
    "                np.mean(npy_predict_cnn[:, :, 9], axis=1)[:, np.newaxis],\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        if save_path != \"\":\n",
    "            with open(save_path + \"cnn_predict_reshaped.npy\", \"wb\") as f:\n",
    "                np.save(f, np.float32(npy_predict_cnn_reshaped))\n",
    "        return npy_predict_cnn_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19aec477-a5f4-44cc-9ac0-847f17646420",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_utils(\n",
    "    grid_info=grid_info,\n",
    "    input_mean=input_mean,\n",
    "    input_max=input_max,\n",
    "    input_min=input_min,\n",
    "    output_scale=output_scale,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53a08196-76bf-4b3f-8154-29ee95c3bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_to_v2_vars()\n",
    "# do not normalize\n",
    "data.normalize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b33b5f7-e627-43ad-99ba-5c36374ad153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    regexps=[\n",
    "        \"E3SM-MMF.mli.000[12345678]-*-*-*.nc\",  # years 1 through 7\n",
    "        \"E3SM-MMF.mli.0009-01-*-*.nc\",\n",
    "    ],\n",
    "\"\"\"\n",
    "\n",
    "data.set_regexps(\n",
    "    data_split=\"train\",\n",
    "    regexps=[\n",
    "        \"E3SM-MMF.mli.0009-01-*-*.nc\",\n",
    "    ],\n",
    ")\n",
    "# set temporal subsampling\n",
    "data.set_stride_sample(data_split=\"train\", stride_sample=1000)  # もとは7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f07a1027-fa6c-4932-8fe4-6063c15bafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of files to extract data from\n",
    "data.set_filelist(data_split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f549f96-ea0c-4525-8a51-aa060070aa80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
